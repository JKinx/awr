{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awr_configs\n",
    "import learning.awr_agent as awr_agent\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import torch\n",
    "import util.rl_path as rl_path\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = awr_configs.AWR_CONFIGS['LunarLanderContinuous-v2']\n",
    "configs[\"action_std\"] = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "graph = tf.Graph()\n",
    "sess = tf.Session(graph=graph)\n",
    "agent = awr_agent.AWRAgent(env=env, sess=sess, **configs)\n",
    "agent.load_model(\"output/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2 = gym.make(\"LunarLanderContinuous-v2\")\n",
    "graph2 = tf.Graph()\n",
    "sess2 = tf.Session(graph=graph2)\n",
    "agent2 = awr_agent.AWRAgent(env=env2, sess=sess2, **configs)\n",
    "agent2.load_model(\"../awr_ori/output/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(agent, s, action_std):\n",
    "    n = len(s.shape)\n",
    "    s = np.reshape(s, [-1, agent.get_state_size()])\n",
    "\n",
    "    feed = {\n",
    "        agent._s_tf : s\n",
    "    }\n",
    "\n",
    "    run_tfs = [agent._norm_a_pd_tf.parameters[\"loc\"]]\n",
    "\n",
    "    out = agent._sess.run(run_tfs, feed_dict=feed)\n",
    "    loc = torch.tensor(out[0])\n",
    "    \n",
    "    a = np.array(torch.distributions.Normal(loc, scale=action_std).sample().tolist())\n",
    "    \n",
    "    if n == 1:\n",
    "        a = a[0]\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_path(agent, action_std):\n",
    "    path = rl_path.RLPath()\n",
    "\n",
    "    s = agent._env.reset()\n",
    "    s = np.array(s)\n",
    "    path.states.append(s)\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        a = sample_action(agent, s, action_std)\n",
    "        s, r, done, info = agent._step_env(a)\n",
    "        s = np.array(s)\n",
    "\n",
    "        path.states.append(s)\n",
    "        path.actions.append(a)\n",
    "        path.rewards.append(r)\n",
    "        path.logps.append(0)\n",
    "\n",
    "    path.terminate = agent._check_env_termination()\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = []\n",
    "for _ in range(100):\n",
    "    path = rollout_path(agent, 0.01)\n",
    "    returns.append(sum([r * (agent._discount ** i) for i,r in enumerate(path.rewards)]))\n",
    "plt.hist(returns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns2 = []\n",
    "for _ in range(100):\n",
    "    path2 = rollout_path(agent2, 0.01)\n",
    "    returns2.append(sum([r * (agent2._discount ** i) for i,r in enumerate(path2.rewards)]))\n",
    "plt.hist(returns2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(paths[:5000], open(\"../awr/paths.5.half.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data(num_episodes, agent, action_std):\n",
    "    episodes = []\n",
    "    \n",
    "    for _ in tqdm(range(num_episodes)):\n",
    "        path = rollout_path(agent, action_std)\n",
    "        I = np.hstack([np.array(path.states)[:-1], np.array(path.actions)])\n",
    "        R = path.rewards\n",
    "        S2 = np.array(path.states)[1:]\n",
    "        episodes.append((I, R, S2))\n",
    "        \n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datas = []\n",
    "# stds = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "# for std in stds:\n",
    "#     data = gather_data(2000, agent, std)\n",
    "#     datas += data\n",
    "# random.shuffle(datas)\n",
    "\n",
    "# pickle.dump(datas, open(\"data.mixed.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(data, open(\"data.5.pkl\", \"wb\"))\n",
    "data = pickle.load(open(\"data.5.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    \"\"\"Policy class with an epsilon-greedy dqn model\"\"\"\n",
    "    def __init__(self, agent, action_std):\n",
    "        super().__init__()\n",
    "        self.agent = agent\n",
    "        self.action_std = action_std\n",
    "\n",
    "    def forward(self, states):\n",
    "        return sample_action(self.agent, states, self.action_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Q(nn.Module):\n",
    "    \"\"\"Q-network using a NN\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, lr):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.fitted = False\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.state_dim + self.action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Forward\"\"\"\n",
    "        state = torch.tensor(state).cuda().float()\n",
    "        return self.model(state)\n",
    "    \n",
    "    def predict(self, state):\n",
    "        \"\"\"Forward without gradients (used for predictions)\"\"\"\n",
    "        state = torch.tensor(state).cuda().float()\n",
    "        with torch.no_grad():\n",
    "            return self.model(state).squeeze().cpu().numpy()\n",
    "    \n",
    "    def fit(self, state, true_value):\n",
    "        \"\"\"Fit NN with a single backward step\"\"\"\n",
    "        self.fitted = True\n",
    "        state = torch.tensor(state).cuda().float()\n",
    "        true_value = torch.tensor(true_value).cuda().float()\n",
    "        self.optimizer.zero_grad()\n",
    "        out = self(state).squeeze()\n",
    "        loss = self.criterion(out, true_value)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_fitted(sklearn_regressor):\n",
    "    \"\"\"Helper function to determine if a regression model from scikit-learn\n",
    "    has ever been `fit`\"\"\"\n",
    "    return hasattr(sklearn_regressor, 'n_outputs_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = data\n",
    "terminals = []\n",
    "for el in datas:\n",
    "    terminal = np.ones(len(el[0]))\n",
    "    terminal[-1] = 0\n",
    "    terminals.append(terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas2 = []\n",
    "for el,terminal in zip(datas, terminals):\n",
    "    datas2.append((el[0],el[1],el[2],terminal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FittedQEvaluation(object):\n",
    "    def __init__(self, regressor=None):\n",
    "        self.regressor = regressor or ExtraTreesRegressor()\n",
    "        self.tree_regressor = regressor is None\n",
    "        \n",
    "    def regressor_fitted(self):\n",
    "        if self.tree_regressor:\n",
    "            return is_fitted(self.regressor)\n",
    "        else:\n",
    "            return self.regressor.fitted\n",
    "        \n",
    "    def Q(self, state_actions):\n",
    "        \"\"\"Return the Q function estimate of `states` for each action\"\"\"    \n",
    "        if not self.regressor_fitted():\n",
    "            return np.zeros(state_actions.shape[0])\n",
    "        return self.regressor.predict(state_actions)\n",
    "\n",
    "    def fit_Q(self, eval_policy, episodes, num_iters=100, discount=0.95):        \n",
    "        batches = []\n",
    "        batch_len = len(episodes) // 10\n",
    "        \n",
    "        for i in range(10):\n",
    "            Is = []\n",
    "            S2s = []\n",
    "            Rs = []\n",
    "            Ts = []\n",
    "\n",
    "            for I,R,S2,T in episodes[i * batch_len : (i + 1) * batch_len]:\n",
    "                Is.append(I)\n",
    "                Rs.append(R)\n",
    "                S2s.append(S2)\n",
    "                Ts.append(T)\n",
    "            \n",
    "            batches.append((np.concatenate(Is, 0), np.concatenate(Rs, 0),\n",
    "                            np.concatenate(S2s, 0), np.concatenate(Ts, 0)))\n",
    "        \n",
    "        for i in tqdm(range(num_iters)):\n",
    "            ins = []\n",
    "            outs = []\n",
    "            for (Is, Rs, S2s, Ts) in batches:\n",
    "                ins.append(Is)\n",
    "                pi_S2s = eval_policy(S2s)\n",
    "                S2pi_S2s = np.hstack([S2s, pi_S2s])\n",
    "                Os = Rs + discount * (Ts * self.Q(S2pi_S2s))\n",
    "                outs.append(Os)\n",
    "            for (Is, Os) in zip(ins, outs):\n",
    "                self.regressor.fit(Is, Os)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnn = Q(agent.get_state_size(), agent.get_action_size(), 0.001).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FQE = FittedQEvaluation(qnn)\n",
    "policy = Policy(agent, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FQE.fit_Q(policy, datas2, 200, agent._discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals0 = []\n",
    "for _ in tqdm(range(100)):\n",
    "    path = rollout_path(agent, 0.5)\n",
    "    true = sum([r * (agent._discount ** i) for i,r in enumerate(path.rewards)])\n",
    "    pred = FQE.regressor.predict(np.hstack([path.states[0], path.actions[0]]).reshape(1,-1))\n",
    "    vals0.append([true, pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter([val[0] for val in vals0], [val[1] for val in vals0], color=\"r\")\n",
    "# plt.hold()\n",
    "# plt.scatter([val[0] for val in vals1], [val[1] for val in vals1], color=\"b\")\n",
    "plt.xlabel(\"True Value\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "plt.title(\"0.5 (200) 0.5\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
