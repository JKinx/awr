{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awr_configs\n",
    "import learning.awr_agent as awr_agent\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import torch\n",
    "import util.rl_path as rl_path\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = awr_configs.AWR_CONFIGS['LunarLanderContinuous-v2']\n",
    "configs[\"action_std\"] = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "graph = tf.Graph()\n",
    "sess = tf.Session(graph=graph)\n",
    "agent = awr_agent.AWRAgent(env=env, sess=sess, **configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from output/model.ckpt\n",
      "Model loaded from: output/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "agent.load_model(\"output/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(agent, s, action_std):\n",
    "    n = len(s.shape)\n",
    "    s = np.reshape(s, [-1, agent.get_state_size()])\n",
    "\n",
    "    feed = {\n",
    "        agent._s_tf : s\n",
    "    }\n",
    "\n",
    "    run_tfs = [agent._norm_a_pd_tf.parameters[\"loc\"]]\n",
    "\n",
    "    out = agent._sess.run(run_tfs, feed_dict=feed)\n",
    "    loc = torch.tensor(out[0])\n",
    "    \n",
    "    a = np.array(torch.distributions.Normal(loc, scale=action_std).sample().tolist())\n",
    "    \n",
    "    if n == 1:\n",
    "        a = a[0]\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_path(agent, action_std):\n",
    "    path = rl_path.RLPath()\n",
    "\n",
    "    s = agent._env.reset()\n",
    "    s = np.array(s)\n",
    "    path.states.append(s)\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        a = sample_action(agent, s, action_std)\n",
    "        s, r, done, info = agent._step_env(a)\n",
    "        s = np.array(s)\n",
    "\n",
    "        path.states.append(s)\n",
    "        path.actions.append(a)\n",
    "        path.rewards.append(r)\n",
    "\n",
    "    path.terminate = agent._check_env_termination()\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data(num_episodes, agent, action_std):\n",
    "    episodes = []\n",
    "    \n",
    "    for _ in tqdm(range(num_episodes)):\n",
    "        path = rollout_path(agent, action_std)\n",
    "        I = np.hstack([np.array(path.states)[:-1], np.array(path.actions)])\n",
    "        R = path.rewards\n",
    "        S2 = np.array(path.states)[1:]\n",
    "        episodes.append((I, R, S2))\n",
    "        \n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b81175a7be74eacb9684a02e4ee81fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = gather_data(10000, agent, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(data, open(\"data.pkl\", \"wb\"))\n",
    "# data = pickle.load(open(\"data.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    \"\"\"Policy class with an epsilon-greedy dqn model\"\"\"\n",
    "    def __init__(self, agent, action_std):\n",
    "        super().__init__()\n",
    "        self.agent = agent\n",
    "        self.action_std = action_std\n",
    "\n",
    "    def forward(self, states):\n",
    "        return sample_action(self.agent, states, self.action_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Q(nn.Module):\n",
    "    \"\"\"Q-network using a NN\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, lr):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.state_dim + self.action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Forward\"\"\"\n",
    "        state = torch.tensor(state).cuda().float()\n",
    "        return self.model(state)\n",
    "    \n",
    "    def predict(self, state):\n",
    "        \"\"\"Forward without gradients (used for predictions)\"\"\"\n",
    "        state = torch.tensor(state).cuda().float()\n",
    "        with torch.no_grad():\n",
    "            return self.model(state)\n",
    "    \n",
    "    def fit(self, state, true_value):\n",
    "        \"\"\"Fit NN with a single backward step\"\"\"\n",
    "        state = torch.tensor(state).cuda().float()\n",
    "        true_value = torch.tensor(true_value).cuda().float()\n",
    "        self.optimizer.zero_grad()\n",
    "        out = self(state).squeeze()\n",
    "        loss = self.criterion(out, true_value)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FittedQEvaluation(object):\n",
    "    def __init__(self, regressor=None):\n",
    "        self.regressor = regressor or ExtraTreesRegressor()\n",
    "        \n",
    "    def Q(self, state_actions):\n",
    "        \"\"\"Return the Q function estimate of `states` for each action\"\"\"\n",
    "        return self.regressor.predict(state_actions)\n",
    "\n",
    "    def fit_Q(self, eval_policy, episodes, num_iters=100, discount=0.95):\n",
    "        Is = []\n",
    "        S2s = []\n",
    "        Rs = []\n",
    "        \n",
    "        batches = []\n",
    "        batch_len = len(episodes) // 10\n",
    "        \n",
    "        for i in range(10):\n",
    "            Is = []\n",
    "            S2s = []\n",
    "            Rs = []\n",
    "\n",
    "            for I,R,S2 in episodes[i * batch_len : (i + 1) * batch_len]:\n",
    "                Is.append(I)\n",
    "                Rs.append(R)\n",
    "                S2s.append(S2)\n",
    "            \n",
    "            batches.append((np.concatenate(Is, 0), np.concatenate(Rs, 0), np.concatenate(S2s, 0)))\n",
    "        \n",
    "        for i in tqdm(range(num_iters)):\n",
    "            for (Is, Rs, S2s) in batches:\n",
    "                pi_S2s = eval_policy(S2s)\n",
    "                S2pi_S2s = np.hstack([S2s, pi_S2s])\n",
    "                Os = Rs + discount * self.Q(S2pi_S2s).cpu().numpy().reshape(-1)\n",
    "                self.regressor.fit(Is, Os)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnn = Q(agent.get_state_size(), agent.get_action_size(), 0.001).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "FQE = FittedQEvaluation(qnn)\n",
    "policy = Policy(agent, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a164e05ea0a64ccdbccc83df6153355d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "FQE.fit_Q(policy, data, 200, agent._discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals0 = []\n",
    "\n",
    "for _ in tqdm(range(100)):\n",
    "    path = rollout_path(agent, 0.1)\n",
    "    true = sum([r * (agent._discount ** i) for i,r in enumerate(path.rewards)])\n",
    "    pred = FQE.regressor.predict(np.hstack([path.states[0], path.actions[0]]).reshape(1,-1))[0].item()\n",
    "    vals0.append([true, pred])\n",
    "\n",
    "# vals1 = []\n",
    "# for _ in tqdm(range(500)):\n",
    "#     path = rollout_path(agent, 0.4)\n",
    "#     true = sum([r * (agent._discount ** i) for i,r in enumerate(path.rewards)])\n",
    "#     pred = FQE.Q(np.hstack([path.states[0], path.actions[0]]).reshape(1,-1))[0]\n",
    "#     vals1.append([true, pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter([val[0] for val in vals0], [val[1] for val in vals0], color=\"r\")\n",
    "# plt.hold()\n",
    "# plt.scatter([val[0] for val in vals1], [val[1] for val in vals1], color=\"b\")\n",
    "plt.xlabel(\"True Value\")\n",
    "plt.ylabel(\"Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
